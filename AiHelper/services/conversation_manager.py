from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from datetime import datetime
import os
import json
from typing import Dict, List, Optional
from dotenv import load_dotenv

load_dotenv()

class ConversationManager:
    """
    Gerenciador de conversas com RAG usando LangChain.
    MantÃ©m contexto da conversa e histÃ³rico por usuÃ¡rio.
    """
    
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.embeddings = OpenAIEmbeddings(api_key=self.api_key)
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            api_key=self.api_key
        )
        
        # Armazena vectorstores e histÃ³ricos por usuÃ¡rio
        self.user_vectorstores: Dict[str, FAISS] = {}
        self.user_histories: Dict[str, List[Dict]] = {}
        
        # DiretÃ³rios para persistÃªncia
        self.storage_dir = "storage/conversations"
        self.vectorstore_dir = f"{self.storage_dir}/vectorstores"
        self.history_dir = f"{self.storage_dir}/histories"
        
        os.makedirs(self.vectorstore_dir, exist_ok=True)
        os.makedirs(self.history_dir, exist_ok=True)
        
    def _get_user_id(self, numero: str) -> str:
        """Extrai ID do usuÃ¡rio do nÃºmero de telefone"""
        return numero.split("@")[0] if "@" in numero else numero
    
    def _load_user_history(self, user_id: str) -> List[Dict]:
        """Carrega histÃ³rico de conversa do usuÃ¡rio"""
        history_file = f"{self.history_dir}/{user_id}.json"
        if os.path.exists(history_file):
            with open(history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    def _save_user_history(self, user_id: str):
        """Salva histÃ³rico de conversa do usuÃ¡rio"""
        history_file = f"{self.history_dir}/{user_id}.json"
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(self.user_histories.get(user_id, []), f, ensure_ascii=False, indent=2)
    
    def _get_or_create_vectorstore(self, user_id: str) -> FAISS:
        """ObtÃ©m ou cria vectorstore para o usuÃ¡rio"""
        if user_id in self.user_vectorstores:
            return self.user_vectorstores[user_id]
        
        vectorstore_path = f"{self.vectorstore_dir}/{user_id}"
        
        # Tenta carregar vectorstore existente
        if os.path.exists(f"{vectorstore_path}/index.faiss"):
            try:
                vectorstore = FAISS.load_local(
                    vectorstore_path,
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                self.user_vectorstores[user_id] = vectorstore
                print(f"âœ… Vectorstore carregado para usuÃ¡rio {user_id}")
                return vectorstore
            except Exception as e:
                print(f"âš ï¸ Erro ao carregar vectorstore: {e}. Criando novo...")
        
        # Cria novo vectorstore com documento inicial
        initial_doc = f"InÃ­cio da conversa com o usuÃ¡rio {user_id} em {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        vectorstore = FAISS.from_texts(
            [initial_doc],
            self.embeddings,
            metadatas=[{"timestamp": datetime.now().isoformat(), "type": "system"}]
        )
        
        # Salva vectorstore
        vectorstore.save_local(vectorstore_path)
        self.user_vectorstores[user_id] = vectorstore
        
        print(f"âœ… Novo vectorstore criado para usuÃ¡rio {user_id}")
        return vectorstore
    
    def _get_chat_history(self, user_id: str, limit: int = 10) -> List:
        """ObtÃ©m histÃ³rico de chat como lista de mensagens LangChain"""
        if user_id not in self.user_histories:
            self.user_histories[user_id] = self._load_user_history(user_id)
        
        history = self.user_histories[user_id]
        messages = []
        
        # Converte Ãºltimas mensagens para formato LangChain
        for msg in history[-limit:]:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            else:
                messages.append(AIMessage(content=msg["content"]))
        
        return messages
    
    def add_message_to_context(self, user_id: str, message: str, is_user: bool = True):
        """Adiciona mensagem ao contexto vetorial"""
        vectorstore = self._get_or_create_vectorstore(user_id)
        
        timestamp = datetime.now().isoformat()
        role = "user" if is_user else "assistant"
        
        # Adiciona ao vectorstore
        vectorstore.add_texts(
            [message],
            metadatas=[{
                "timestamp": timestamp,
                "role": role,
                "type": "message"
            }]
        )
        
        # Salva vectorstore atualizado
        vectorstore_path = f"{self.vectorstore_dir}/{user_id}"
        vectorstore.save_local(vectorstore_path)
        
        # Adiciona ao histÃ³rico
        if user_id not in self.user_histories:
            self.user_histories[user_id] = []
        
        self.user_histories[user_id].append({
            "role": role,
            "content": message,
            "timestamp": timestamp
        })
        
        self._save_user_history(user_id)
    
    def get_relevant_context(self, user_id: str, query: str, k: int = 5) -> List[str]:
        """Recupera contexto relevante do histÃ³rico"""
        vectorstore = self._get_or_create_vectorstore(user_id)
        
        # Busca documentos relevantes
        docs = vectorstore.similarity_search(query, k=k)
        
        return [doc.page_content for doc in docs]
    
    def generate_response(self, numero: str, user_message: str) -> str:
        """
        Gera resposta usando RAG com contexto da conversa.
        
        Args:
            numero: NÃºmero do WhatsApp do usuÃ¡rio
            user_message: Mensagem do usuÃ¡rio
            
        Returns:
            Resposta gerada pela IA
        """
        user_id = self._get_user_id(numero)
        
        print(f"ğŸ¤– Gerando resposta para usuÃ¡rio {user_id}")
        print(f"ğŸ“ Mensagem: {user_message}")
        
        # Adiciona mensagem do usuÃ¡rio ao contexto
        self.add_message_to_context(user_id, user_message, is_user=True)
        
        # ObtÃ©m contexto relevante
        relevant_context = self.get_relevant_context(user_id, user_message, k=5)
        print(f"ğŸ“š Contexto relevante recuperado: {len(relevant_context)} mensagens")
        
        # ObtÃ©m histÃ³rico de chat
        chat_history = self._get_chat_history(user_id, limit=10)
        
        # Cria prompt com contexto
        prompt = ChatPromptTemplate.from_messages([
            ("system", """VocÃª Ã© um assistente prestativo e amigÃ¡vel. 
            Use o contexto da conversa anterior para dar respostas mais personalizadas e coerentes.
            Se o usuÃ¡rio fizer referÃªncia a algo mencionado antes, use esse contexto.
            
            Contexto relevante da conversa:
            {context}
            """),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}")
        ])
        
        # Cria chain
        chain = prompt | self.llm
        
        # Gera resposta
        try:
            response = chain.invoke({
                "context": "\n".join(relevant_context),
                "chat_history": chat_history,
                "question": user_message
            })
            
            resposta_texto = response.content
            
            # Adiciona resposta ao contexto
            self.add_message_to_context(user_id, resposta_texto, is_user=False)
            
            print(f"âœ… Resposta gerada com sucesso")
            return resposta_texto
            
        except Exception as e:
            print(f"âŒ Erro ao gerar resposta: {e}")
            raise
    
    def get_conversation_summary(self, numero: str, limit: int = 10) -> List[Dict]:
        """Retorna resumo da conversa do usuÃ¡rio"""
        user_id = self._get_user_id(numero)
        history = self.user_histories.get(user_id, [])
        return history[-limit:]
    
    def clear_user_context(self, numero: str):
        """Limpa contexto de um usuÃ¡rio especÃ­fico"""
        user_id = self._get_user_id(numero)
        
        # Remove do cache
        if user_id in self.user_vectorstores:
            del self.user_vectorstores[user_id]
        if user_id in self.user_histories:
            del self.user_histories[user_id]
        
        print(f"ğŸ—‘ï¸ Contexto do usuÃ¡rio {user_id} limpo")


# InstÃ¢ncia global
conversation_manager = ConversationManager()
